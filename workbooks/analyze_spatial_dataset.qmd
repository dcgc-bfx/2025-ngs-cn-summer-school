---
title: "ðŸ§¬ Spatial Transcriptomics Analysis (part 2)"
format: html
---

# Welcome to the NGS-CN Summer School on Spatial Transcriptomics Analysis!

Spatial transcriptomics allows us to measure gene expression across tissue sections while preserving spatial information, helping us understand not just **what** genes are active, but **where** they are active.

In this practical session, we will explore how to analyze spatial transcriptomics data using **R** and **RStudio**. Whether you are new to spatial data or already familiar with single-cell workflows, this guide will walk you through the essential steps.

Feel free to use your personal workbook to make notes, track your progress, and revisit it after this workshop.

---

In this **second part**, we will cover key steps in the analysis workflow, including quality control, preprocessing, clustering and visualisation, cluster annotation, spatial clustering, and integration with single-cell transcriptomics data.

## Setting up your R environment 

Before diving into the data, we need to load a few important R packages. These libraries will provide the tools we need for analysis and visualization.

```{r}
#| label: setup
#| message: false

library(Seurat)     # Main package for single-cell and spatial transcriptomics analysis
library(SeuratWrappers) # Add-ons for Seurat
library(patchwork)  # Helps combine multiple ggplot2 plots into one figure
library(tidyverse)  # Essential collection of R packages, including ggplot2, dplyr, magrittr
library(grid)       # For other plots
library(viridis)    # Our continuous colour palette

# Colours
cal_pal50 = c("#Fa1a8e", "#009B7D", "#ff9933", "#7083a4", "#ffce45", "#015e05", 
              "#fedf25", "#d2b48c", "#bb55e1", "#6ec5ab", "#5d57af", "#143341", 
              "#761445", "#d65b5a", "#94043a", "#e7a6cd", "#204519", "#87afd1", 
              "#9b9a9d", "#f95b70", "#83c874", "#808080", "#452b5b", "#ecb100", 
              "#f46124", "#525252", "#4c84a3", "#00bfff", "#01b4c6", "#174d79", 
              "#a6a0f2", "#76facc", "#8491be", "#a32a2f", "#1c8859", "#2cc012", 
              "#35782b", "#9c6755", "#3b3960", "#eeb0a1", "#3e1e13", "#0064c3", 
              "#d81e4a", "#74646c", "#f675da", "#ffce45", "#ec7014", "#e50000", 
              "#000000", "#a4527c", "#041859")

# ggplot2 defaults
options(ggplot2.discrete.colour=cal_pal50)
options(ggplot2.discrete.fill=cal_pal50)
options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill="viridis")
options(ggplot2.binned.colour="viridis")
options(ggplot2.binned.fill="viridis")

# Hook to measure the computation time of a chunk
# Activate with '#| timeit: true'. Deactivate with '#| timeit: null'
knitr::knit_hooks$set(timeit = function(before, options, envir) {
  if(before) {
    ## code to be run before a chunk
    tictoc::tic()
  } else {
    ## code to be run after a chunk
    elapsed = tictoc::toc()$toc
    print(paste0("Execution took ", elapsed, " seconds"))
  }
})
```

# Loading the dataset

```{r}
#| label: dataset

sc = Load10X_Spatial("datasets/visiumhd_mouse_brain", bin.size=c(8, 16), assay="RNA")
sc$orig.ident = "brain"
Idents(sc) = "brain"
```

# Quality control

## Why quality control?

In spatial transcriptomics (and any sequencing-based assay), not all bins are equally informative. Quality Control (QC) is used to identify and remove low-quality bins from the dataset before further analysis. 

**Low-quality bins** might be the result of:  

* Problems during reverse transcription, PCR amplification or library preparation  
* Cell damage during the experiment, e.g. when the tissue already contains a lot of dead or dying cells  
* Very few transcripts overall, e.g. in empty regions  
* Unusually high numbers of transcripts, possibly doublets or noise  

**Why does this matter?**  
Low-quality bins can distort the results. For instance, they might cluster separately and be falsely interpreted as novel biological states or cell types. Performing QC early helps ensure the reliability of downstream steps like normalization, clustering, and cell type annotation.

## QC metrics at a glance

We will use three common metrics to flag low-quality bins:

- **Number of counts (UMIs):** Measures total expression in a bin. Too low = empty; too high = potential artifacts.
- **Number of detected genes:** Captures the diversity of expression in a bin. Low gene counts might mean poor capture.
- **Mitochondrial gene content:** High levels often indicate stressed or dying cells.

These metrics will be assessed both statistically and spatially to get an idea of the sample quality and to identify thresholds for filtering.

## Number of Counts per Bin

The total number of UMIs (counts) per bin gives a sense of overall transcript abundance.

```{r}
#| label: qc_1

p1 = VlnPlot(sc, features="nCount_RNA.008um", pt.size=0, layer="counts") + NoLegend()
p2 = SpatialFeaturePlot(sc, features="nCount_RNA.008um")
p1 | p2

summary(sc$nCount_RNA.008um)
```

Look for bins with very few or very many counts. Bins with <30 counts or >2000 counts are suspicious and flagged for removal. 

::: {.callout-tip}
### Bins flagged here should appear randomly across the tissue #####
If they form a distinct pattern, reconsider the threshold â€” you might be removing biologically relevant data.
:::

```{r}
#| label: qc_2

filtered_bins_counts = WhichCells(sc, expression=nCount_RNA.008um < 30 | nCount_RNA.008um > 2000)
SpatialDimPlot(sc, cells.highlight=filtered_bins_counts, cols.highlight=c("#FFFF00", "grey50")) + NoLegend()
```

::: {.callout-note}
### Find your optimal threshold
Play around with the thresholds in R. Which threshold would you find best?
:::

## Number of Genes per Bin

This tells us how many genes are detected per bin â€” essentially a measure of complexity.

```{r}
#| label: qc_3

p1 <- VlnPlot(sc, features="nFeature_RNA.008um", pt.size=0, layer="counts") + NoLegend()
p2 <- SpatialFeaturePlot(sc, features="nFeature_RNA.008um")
p1 | p2

summary(sc$nFeature_RNA.008um)
```

```{r}
#| label: qc_4

filtered_bins_genes = WhichCells(sc, expression = nFeature_RNA.008um < 30 | nFeature_RNA.008um > 1000)
SpatialDimPlot(sc, cells.highlight=filtered_bins_genes, cols.highlight=c("#FFFF00", "grey50")) + NoLegend()
```

## Mitochondrial content

Mitochondrial RNA (mtRNA) often increases when cells are stressed or dying. A high percentage is a red flag.

We identify mitochondrial genes using a prefix (^mt-) and calculate their percentage per bin:

```{r}
#| label: qc_5

sc = PercentageFeatureSet(sc, pattern="^mt-", col.name="pMito_RNA.008um") 

p1 = VlnPlot(sc, features="pMito_RNA.008um", pt.size=0, layer="counts") + NoLegend()
p2 = SpatialFeaturePlot(sc, features="pMito_RNA.008um") + NoLegend()
p1 | p2
```

```{r}
#| label: qc_6

filtered_bins_mito = WhichCells(sc, expression=pMito_RNA.008um>30)
SpatialDimPlot(sc, cells.highlight=filtered_bins_mito, cols.highlight=c("#FFFF00", "grey50")) + NoLegend()
```

## Filtering of bins

Once you have reviewed the metrics and decided on thresholds, remove poor-quality bins using subset():

Before filtering:

```{r}
#| label: qc_7

dim(sc[["RNA.008um"]])
```

Apply the filters:

```{r}
#| label: qc_8

sc = subset(sc, nCount_RNA.008um < 30 | nCount_RNA.008um > 2000, invert=TRUE) %>% suppressWarnings()
sc = subset(sc, nFeature_RNA.008um < 30 | nFeature_RNA.008um > 1000, invert=TRUE) %>% suppressWarnings()
sc = subset(sc, pMito_RNA.008um > 30, invert=TRUE) %>% suppressWarnings()
sc[["RNA.008um"]]
```

## Filtering of genes

Genes expressed in only a few bins can add noise. First, we count how many bins each gene is detected in:

```{r}
#| label: qc_9

counts = GetAssayData(sc, layer="counts")
genes_expressed = rowSums(counts > 0)
head(genes_expressed, 5)
summary(genes_expressed)
```

Let us quickly summarize these numbers. Most of the genes are expressed in at least 30 bins, on average in 4063 bins and maximum in 283994 bins.

```{r}
#| label: qc_10

summary(genes_expressed)
```

Filter out genes expressed in â‰¤10 bins:

```{r}
#| label: qc_11

genes_expressed = genes_expressed[genes_expressed > 10]
length(genes_expressed)
```

Then subset the Seurat object:

```{r}
#| label: qc_12

sc[["RNA.008um"]]
sc[["RNA.008um"]] = subset(sc[["RNA.008um"]], features=names(genes_expressed)) %>% suppressWarnings()
sc[["RNA.008um"]]
```

::: {.callout-tip}
### Best practices
* Avoid "one-size-fits-all" thresholds; adjust based on sample type, bin size, and sequencing depth
* Make sure to check the spatial distribution of filtered bins to avoid removing spatial patterns
* At the beginning, run a full analysis (until clustering) without filtering 
  * Low-quality bins often cluster together
  * Plot QC metrics per cluster and identify clusters without or with ambiguous cell type
  * Then adjust filtering criteria accordingly and repeat
* Advanced: use tools that include neighboring bins (e.g [SpotSweeper](https://github.com/MicTott/SpotSweeper))
:::

::: {.callout-note}
### Questions  
What might cause unusually high count bins?  
Why might mitochondrial content be misleading in some tissues (e.g., muscle or brain)?  
What are the trade-offs of stricter vs. more lenient filtering?  
:::

# Preprocessing

After removing low-quality bins, we now prepare the dataset for downstream analysis. This **preprocessing pipeline** includes:

1. Normalization of raw counts
2. Identification of highly variable genes (HVGs)
3. Centering and scaling of those HVGs
4. Optional sketching of the dataset for speed (HVGs)
5. Dimensionality reduction (PCA) based on the HVGs

Each step ensures that only informative, reliable data is passed into clustering and annotation workflows.

## Normalization

Raw counts vary widely across bins due to differences in sequencing depth (library size). To make counts **comparable across bins**, we perform normalization.

Note that normalization methods for spatial data are still being developed and evaluated. For this workshop, we apply log-normalization. 

**How does it work?**  

* Per cell, the count for each gene is divided by the total count for that cell, and then multiplied by 10,000  
* These "counts per 10,000" are then log-transformed using the natural logarithm (with a pseudocount of 1)  

This is implemented in Seurat with `NormalizeData()`:

```{r}
#| label: preprocessing_1

sc = NormalizeData(sc, normalization.method="LogNormalize", scale.factor=10000)
sc[["RNA.008um"]]
```

This creates a new layer in the assay object called `data` that contains the normalized counts. You can inspect the normalized values like so:

```{r}
#| label: preprocessing_2

data = GetAssayData(sc, layer="data")
data[1:10, 1:10]
```

## Identifying Highly Variable Genes (HVGs)

Not all genes are equally informative. Experience shows that 1,000-3,000 genes with the highest bin-to-bin variation are often sufficient to describe the global structure of a dataset. 

Highly variable genes are typically the genes that show strong variation across bins and are more likely to distinguish biological states. Housekeeping genes with similar expression profiles across all bins, or genes with minor expression differences, might add random noise and mask relevant changes during downstream analysis. 

Here, we determine the top 1000 HVGs: 

```{r}
#| label: preprocessing_3

sc = FindVariableFeatures(sc, selection.method="vst", nfeatures=1000)
```

Note that 1,000 HGVs represents the lower end of the possible range. However, for demonstration purposes and to save memory, we use 1,000 in this example.

Get the top 10 HVGs and plot them:

```{r}
#| label: preprocessing_4

top10 = VariableFeatures(sc) %>% head(10)
top10

p = VariableFeaturePlot(sc, selection.method="vst")
p = LabelPoints(plot=p, points=top10, repel=TRUE)
p
```

## Data scaling

Gene expression ranges vary; some genes have inherently higher values. Centering and scaling ensures each gene contributes equally during the analysis. By centering (mean expression of 0) and scaling (standard deviation of 1), all genes will be on the same scale which makes them comparable. 

We use the `ScaleData` function to center and scale the HVGs:

```{r}
#| label: preprocessing_5

sc = ScaleData(sc, features=VariableFeatures(sc))
GetAssay(sc, assay="RNA.008um")
```

This creates a new layer in the assay object called `scale.data` that contains the scaled counts:

```{r}
#| label: preprocessing_6

scale_data = GetAssayData(sc, layer="scale.data")
scale_data[1:5, 1:5]
```

## Creating a sketch assay

Large datasets can slow down clustering and UMAP. A sketch assay creates a reduced representation while preserving rare cell populations.

Instead of random sampling, Seurat uses a leverage score to keep influential or rare bins. We sketch 50,000 bins:

```{r}
#| label: preprocessing_7

sc = SketchData(sc, ncells=50000, method="LeverageScore", sketched.assay="sketch", features=VariableFeatures(sc))
sc
```

When we print the Seurat object, we see that the default is now the sketch assay. We can also check that it is indeed smaller:

```{r}
#| label: preprocessing_8

GetAssay(sc, assay="sketch")
```

Repeat HVG selection and scaling for the sketch:

```{r}
#| label: preprocessing_9

sc = FindVariableFeatures(sc, selection.method="vst", nfeatures=1000)
sc = ScaleData(sc, features=VariableFeatures(sc))
GetAssay(sc, assay="sketch")
```

::: {.callout-tip}
### Tip
Sketching is ideal for exploration on (very) large datasets. If your computing machine is super powerful or your dataset reasonably small, you might not need it. 
:::

## Dimensionality reduction

With ~20,000 genes, the data is too high-dimensional for visualization or clustering. At this point of the analysis, we have already reduced the dimensionality of the dataset to 1,000 HVGs. The biological manifold however can be described by far fewer dimensions than the number of (variable) genes, since expression profiles of different genes are correlated if they are involved in the same biological process. 

We use Principal Component Analysis (PCA) to reduce the dimensionality. PCA finds new axes (PCs) that capture the most variance in the dataset, and projects the data onto these axes. The resulting meta-variables are called principal components (PCs).

We use the `RunPCA` function to calculate the top 50 principle components (PCs) for the data. This creates a new dimensionality reduction object called `pca.sketch`:

```{r}
#| label: preprocessing_10

sc = RunPCA(sc, reduction.name="pca.sketch", features=VariableFeatures(sc), npcs=50, nfeatures.print=5)
Reductions(sc, slot="pca.sketch")
```

Visualize first two PCs:

```{r}
#| label: preprocessing_11

DimPlot(sc, reduction="pca.sketch", dims=c(1, 2))
```

In PCA, genes with the largest variation between bins will have the biggest influence on the PCs. The first PC captures the greatest variance across the dataset. The next PC captures the greatest remaining amount of variance, and so on. This way, the top PCs are likely to represent the biological signal where multiple genes are affected by the same biological processes in a coordinated way. In contrast, random technical or biological noise that affects each gene independently are contained in later PCs. 

Downstream analysis can be restricted to the top PCs. We need to decide how many PCs we want to use for our analysis. The following elbow plot shows PCs ranked based on the variance they explain. 

Look for the "elbow" â€” a point where adding more PCs gives diminishing returns:

```{r}
#| label: preprocessing_12

ElbowPlot(sc, reduction="pca.sketch", ndims=50)
```

You can also check which genes contribute most to each PC:

```{r}
#| label: preprocessing_13

VizDimLoadings(sc, reduction="pca.sketch", dims=1:4, nfeatures=10, balanced=TRUE)
```

::: {.callout-tip}
## Learnings
Helpful YouTube video on PCA in the context of gene expression analysis is (here)[https://www.youtube.com/watch?v=FgakZw6K1QQ]
:::

::: {.callout-note}
### Question
How many PCs would you choose for further analysis? 
:::

# Clustering and visualization

In the following, we **cluster** spatial bins based on their gene expression profiles and use **UMAP** (Uniform Manifold Approximation and Projection) to visualise the results in two dimensions.

**Clustering** is typically done using unsupervised algorithms (e.g., Louvain or Leiden) that group bins based on similarities across the full gene expression space. This helps to identify regions within the tissue that share similar transcriptional signatures, which may correspond to distinct anatomical structures or functional states. 

**UMAP** is a dimensionality reduction technique that projects high-dimensional data into a lower-dimensional space, in this case 2D, to reveal underlying structure. In UMAP plots, each bin is shown as a point, allowing us to detect substructures or transitions within clusters, identify outliers or ambiguous regions, communicate findings visually and intuitively. 

Clusters are discrete groupings assigned algorithmically based on expression similarity; each bin belongs to exactly one cluster. UMAP clouds, in contrast, are continuous visual projections. While clusters often appear as distinct clouds in UMAP space, overlaps can occur, especially in biologically complex areas. UMAP may reveal gradual transitions or relationships that clustering alone cannot capture.

## Clustering of the sketched dataset

Seurat uses a graph-based approach for clustering. In short, Seurat first constructs a graph by identifying the k-nearest neighbors for each bin in the PCA space based on Euclidean distance (NN). It then builds a refined version of the neighbor graph by considering shared neighbors between bins (SNN). Finally, a clustering algorithm is applied to the graph to identify groups of similar bins. These groups are then considered distinct cell populations. 

We begin by creating a nearest-neighbor graph using the `FindNeighbors` function in Seurat. We use the first 30 PCs as input and set the number of nearest neighbors to 20:

```{r}
#| label: clustering_1

sc = FindNeighbors(sc, reduction="pca.sketch", dims=1:30, k.param=20)
Graphs(sc)
```

After constructing the nearest-neighbor graph, we apply the Leiden algorithm to perform the clustering. The Leiden algorithm is an improvement over the Louvain method and works by optimizing modularity. It groups bins into clusters such that bins within the same cluster are more connected to each other than to bins in other clusters.

We call the `FindClusters` function with `algorithm=4`, which tells Seurat to use the Leiden algorithm. We also need to specify the resolution parameter. This parameter controls the granularity of the clustering:

* Higher resolution values lead to more clusters  
* Lower resolution values lead to fewer clusters  

We will start with a resolution of 2, but this can be adjusted later depending on the number of clusters you wish to identify:

```{r}
#| label: clustering_2

sc = FindClusters(sc, resolution=2, algorithm="leiden", random.seed=1)

# Save as extra metadata column
sc$seurat_clusters.sketch = sc$seurat_clusters
```

The clusters are saved as bin metadata column `seurat_clusters.sketch`. Let us summarize the clustering of the sketched dataset:

```{r}
#| label: clustering_3

table(sc$seurat_clusters.sketch)
```

Note that we can provide multiple resolution values to test different clustering resolutions.

::: {.callout-note}
## Assignment
Test different cluster resolutions and observe the number of clusters you receive. 
:::

## UMAP of the sketched dataset

Now, we move on to generating a UMAP plot, which provides a visual representation of the dataset in lower-dimensional space. This helps to visually identify groups of similar cells, making it easier to explore and interpret the data.

We run the `RunUMAP` function and use the first 30 PCs as input. This creates a new dimensionality reduction object called `umap.sketch`:

```{r}
#| label: clustering_4

sc = RunUMAP(sc, reduction="pca.sketch", reduction.name="umap.sketch", dims=1:30, return.model=TRUE)
```

Note that `umap.sketch` is now the default dimensionality reduction. We can color the clustering on the UMAP as well as in spatial context:

```{r}
#| label: clustering_5

p1 = DimPlot(sc, reduction="umap.sketch", label=TRUE) + NoLegend()
p2 = SpatialDimPlot(sc, images="slice1.008um") + NoLegend() + scale_fill_manual(values=cal_pal50)
p1 | p2
```

Here we can clearly see that we only analysed a subsampled dataset.

## Analysing the full dataset

Once we have finished analyzing the sketched dataset, we can project the learned cluster labels, PCA, and UMAP from the sketch assay onto the entire dataset. This allows us to transfer the insights gained from the subsampled dataset to the full dataset, which may have additional complexity.

We use the `ProjectData` function to carry out this projection. Hereâ€™s the code to project the data from the sketch assay to the full dataset:

```{r}
#| label: clustering_6

sc = ProjectData(sc,
                 sketched.assay="sketch", assay="RNA.008um", 
                 sketched.reduction="pca.sketch", full.reduction="pca.008um", 
                 umap.model="umap.sketch",
                 refdata=list(seurat_clusters.008um="seurat_clusters.sketch"),
                 dims=1:30)

# Fix the category levels for bin metadata column seurat_clusters.008um and update the bin identities
sc$seurat_clusters.008um = factor(sc$seurat_clusters.008um, levels=levels(sc$seurat_clusters.sketch))
Idents(sc) = "seurat_clusters.008um"

# Rename UMAP from full.umap.sketch to umap.008um
umap = sc[["full.umap.sketch"]]
Key(umap) = "umap.008"
sc[["umap.008um"]] = umap
sc[["full.umap.sketch"]] = NULL

# Set default assay back to RNA.008um
DefaultAssay(sc) = "RNA.008um"
gc()
```

The Seurat object now contains a full PCA (`pca.008um`), a full UMAP (`umap.008um`) and a full clustering (`seurat_clusters.008um`). We color the clusters of the full dataset on the UMAP as well as in spatial context:

```{r}
#| label: clustering_7

p1 = DimPlot(sc, reduction="umap.008um", label=TRUE) + NoLegend()
p2 = SpatialDimPlot(sc) + NoLegend() + scale_fill_manual(values=cal_pal50)
p1 | p2
```

When dealing with many clusters, some of which may be spatially restricted while others are not, it can be challenging to interpret their spatial distribution. In such cases, highlighting individual clusters can provide more clarity.

We can highlight specific clusters by using the `CellsByIdentities` function to select bins belonging to particular clusters. Here, we highlight clusters 16 and 8:

```{r}
#| label: clustering_8

cluster_bins = CellsByIdentities(sc, idents=c(16, 8))
cluster_bins[["NA"]] = NULL

SpatialDimPlot(sc, cells.highlight=cluster_bins, facet.highlight=TRUE, cols.highlight=c("#FFFF00", "grey50")) + NoLegend()
```

::: {.callout-tip}
## Recommendations

* Clustering  
  * Test different resolution values to find the best clustering solution for your data  
* UMAP  
  * UMAP "cloud" sizes and the distance between them may not be meaningful ast the method has a local notion of distance
  * Helpful explanation on YouTube (here)[https://www.youtube.com/watch?v=eN0wFzBA4Sc]
  * Helpful documentation and option to explore (here)[https://pair-code.github.io/understanding-umap/]  
* Keep in mind that this represents only a portion of the entire dataset, and the full dataset may reveal additional insights
* Take your time. In scRNA-seq analysis, this is perhaps the most time-consuming step of the basic analysis
:::

::: {.callout-note}
## Question   
* How many clusters do we have?    
* What can we already say about the clusters we have now?   
:::

# Cluster annotation

What cell types correspond to the clusters? To identify the cell types corresponding to the clusters, we typically follow these steps:

* **Cluster marker genes**: Use differential gene expression analysis to find genes that are specifically expressed in each cluster. These can serve as markers for the corresponding cell types.
* **Known marker genes**: Visualize the expression of known cell-type markers on the UMAP to help associate clusters with specific cell types.
* **Mapping a reference dataset**: Use external datasets to map cell-type labels onto your data, enhancing the accuracy of your cell-type predictions.

By using these strategies, you can correlate the clusters with biological cell types and better understand the cell composition of your dataset.

## Cluster marker genes

One of the first steps in identifying the cell types of each cluster is to determine cluster-specific marker genes. These are genes that are significantly up- or down-regulated in one cluster compared to all other clusters, which can be used to characterize the biological identity of the cells in each cluster. 

A good clustering of cells typically results in good marker genes. Hence, if you cannot find good marker genes you may need to go back to the start of the workflow and adapt your parameters. Note that we also determine genes that are particularly down-regulated in one cluster, even if these are not marker genes in the classical sense.

Resulting *p*-values are adjusted using the Bonferroni method. However, beware that *p*-values are likely inflated, since both clusters and marker genes were determined based on the same gene expression data, and there ought to be gene expression differences by design. Nevertheless, *p*-values can be used to sort and prioritize marker genes. 

You can use Seurats `FindMarkers` function to identify differentially expressed genes between clusters:

```{r}
#| label: clusterannot_1

markers = FindAllMarkers(object=sc,
                         test.use="wilcox",
                         layer="data",
                         only.pos=FALSE,
                         max.cells.per.ident=1000)
```

::: {.callout-note}
## Assignment

Extract a table of the top 5 marker genes per cluster, ranked by the adjusted *p*-value and log2 foldchange. 
:::

```{r}
#| label: clusterannot_2

# Do this here
# markers_top = ...

# Solution
markers_top = markers %>% 
  dplyr::group_by(cluster) %>% 
  dplyr::arrange(p_val_adj, avg_log2FC) %>% 
  dplyr::slice_head(n=3) %>% 
  dplyr::ungroup() %>% 
  as.data.frame()

gt::gt(markers_top) %>% 
  gt::tab_options(container.height=450)
```

Good marker genes are highly and possibly even only expressed in one cluster as compared to all other clusters. However, sometimes marker genes are also expressed in other clusters, or are declared as marker genes in these clusters, for example cell lineage markers that are shared by different cell sub types. To evaluate marker genes, it is essential to visualize their expression patterns.

```{r}
#| label: clusterannot_3
#| fig-height: 8
#| fig-width: 12

DotPlot(sc, features=markers_top$gene %>% unique()) +
  viridis::scale_color_viridis() + 
  ylab("Cluster") + xlab("") +
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=.5),
        legend.position="bottom") + 
  guides(size=guide_legend(order=1, title="Pct expressed"), color=guide_colorbar(title="Scaled Expression")) + 
  ggtitle("Top markers per cluster (expression scaled)")

DotPlot(sc, features=markers_top$gene %>% unique(), scale=FALSE) + 
  viridis::scale_color_viridis() + 
  ylab("Cluster") + xlab("") + 
  theme(axis.text.x=element_text(angle=90, hjust=1, vjust=.5),
        legend.position="bottom") + 
  guides(size=guide_legend(order=1, title="Pct expressed"), color=guide_colorbar(title="Average Expression")) +
  ggtitle("Top markers per cluster (expression unscaled)")

```

::: {.callout-note}
## Assignment

Take a good look at the dot plot. Which clusters do / do not have good marker genes? 
:::

## Functional enrichment analysis

We can use the identified cluster marker genes to perform functional enrichment analysis. This helps us infer the potential biological identity or function of each cluster. For simplicity, we will use the online tool [Enrichr](https://maayanlab.cloud/Enrichr/) to perform this analysis. 

::: {.callout-note}
## Assignment

Try to find the identity of one cluster! 
:::

## Plotting gene expression

To further explore the results and visualize gene expression across different clusters, we can use various plotting functions.

A **violin plot** allows us to visualize the distribution of gene expression across clusters. For example, we can plot the expression of genes Acta2 and Apoe:

```{r}
#| label: plotting_1

VlnPlot(sc, features=c("Acta2", "Apoe"), pt.size=0, ncol=1) + NoLegend()
```

We can also visualize the gene expression directly on top of the **UMAP plot**. This will show the expression of genes such as Acta2 and Apoe across all cells in the low-dimensional UMAP space:

```{r}
#| label: plotting_2

FeaturePlot(sc, features=c("Acta2", "Apoe"), pt.size=0)
```

Since we are working with spatial data, you can visualize gene expression in the **spatial context**, showing where the genes are expressed on the tissue section:

```{r}
#| label: plotting_3

SpatialFeaturePlot(sc, features=c("Acta2", "Apoe"), pt.size=0)
```

A **dot plot** allows for the visualization of gene expression across multiple clusters for several genes. Here's an example where we plot the expression of 10 genes across clusters 1 to 10:

```{r}
#| label: plotting_4

DotPlot(sc, idents=1:10, 
        features=c("Hap1", "Nrgn", "Snap25", "Camk2n1", "Apoe", "Nap1l5", "Olfm1", "Penk", "Ptgds", "Pcp4"),
        scale=TRUE) + 
  scale_color_viridis() +
  theme(axis.text.x=element_text(angle=45, hjust=1))
```

For a more detailed look, we can create a **heatmap** to visualize the expression of the same genes in a subset of cells. This allows for a more focused analysis of gene expression patterns across specific clusters.

```{r}
#| label: plotting_5

set.seed(1)
cluster_bins = Cells(sc) %>% sample(10000)

DoHeatmap(sc, features=c("Hap1", "Nrgn", "Snap25", "Camk2n1", "Apoe", "Nap1l5", "Olfm1", "Penk", "Ptgds", "Pcp4"), 
          cells=cluster_bins,
          group.colors=cal_pal50) + 
  scale_fill_viridis() +
  guides(colour="none")
```

```{r}
#| label: rm_1

# Remove since not used anymore and we need the memory.
rm(counts, data, scale_data, assay, umap)

# R will automatically clear the memory of freed objects whenever it needs more space.
# But in our experience it is sometimes better to explicitly tell R to do this - especially when working with big objects.
gc()
```

```{r}
knitr::knit_exit()
```

::: {.callout-note}
## Assignment
Visualize cluster marker genes and known marker genes on the UMAP and in spatial context, and identify at least one cell type from it. 
:::

# Spatial clustering and domains

TODODODOD: This takes very long. Two solutions:
- Crop the 8 um dataset to subset already at the beginning
cortex.coordinates <- as.data.frame(read.csv("/brahms/lis/visium_hd/final_mouse/cortex-hippocampus_coordinates.csv"))
cortex <- CreateSegmentation(cortex.coordinates)

object[["cortex"]] <- Overlay(object[["slice1.008um"]], cortex)
cortex <- subset(object, cells = Cells(object[["cortex"]]))
Could also be a good preprocessing step

- Take only the first 10 dimension
- Run this part with 16 um dataset
- Skip one part

So far, we have learned the basics of analyzing spatial (and single-cell) datasets with Seurat and our workflow from above is a good starting point for an analysis.

However, it treats each bin independently and without considering its spatial context. Spatial analysis methods aim to improve this by incorporating the neighborhood of the bins. Several steps of our basic workflow could be adapted accordingly. Additionaly, 

## What is BANKSY?

(BANKSY)[https://github.com/prabhakarlab/Banksy] is a package for the analysis of spatial data. By including neighborhood information, it is able:

- improve noisy data
- distinguish between different cell types depending on the spatial context
- identify spatial domains

The amount of neighborhood information is controlled by the `lambda` parameter:
- With smaller values (e.g. 0.2) BANKSY focuses on the closest neighbors. This is used for cell typing.
- With larger values (e.g. 0.8) BANKSY includes more distant neighbors. This is used for domain finding.

BANKSY is an independent package that uses another framework for analysis. However, the `RunBanksy` function from the `SeuratWrappers` package allows us to run BANKSY on an Seurat object so that we do not have to learn all the details of the BANKSY package.

## Improve clustering with BANKSY

BANKSY can be used to as alternative to our default clustering strategy from before. We set `lambda` to a small value (0.2) so that BANKSY focuses only on the closest neighbors. Additionally, we set the number of neighbors (`k_geom`) to 15.

Note that this step can take a while and is very memory-intensive. 

```{r}
#| label: spatial_1

sc = RunBanksy(sc,
               assay="RNA.008um", assay_name="BANKSY.008um",
               features="variable",
               lambda=0.2, k_geom=15,
               verbose=TRUE)

# Set it as default
DefaultAssay(sc) = "BANKSY.008um"

gc()
```

Now we have created a new assay `BANKSY.008um` that contains the BANKSY results. It is also now the default assay. Important: Do not scale this assay as it will overwrite the BANKSY results.

```{r}
#| label: spatial_2

sc
```

We then use this assay to do PCA dimensionality reduction followed by clustering and UMAP. We will use the first 20 PCs and set the clustering resolution to 0.5. However this is just a starting point and you should try different values to find the best clustering for your data:

```{r}
#| label: spatial_3

sc = RunPCA(sc, reduction.name="banksy_pca.008um", features=rownames(sc[["BANKSY.008um"]]), npcs=15, nfeatures.print=5)
sc = FindNeighbors(sc, reduction="banksy_pca.008um", dims=1:15)
sc = FindClusters(sc, resolution=0.5, algorithm="leiden")
sc = RunUMAP(sc, reduction="banksy_pca.008um", reduction.name="banksy_umap.008um", dims=1:15, return.model=TRUE)

sc$banksy_clusters.008um = sc$seurat_clusters
DefaultAssay(sc) = "RNA.008um"
```

Here are our two analyses plotted side-by-side:

```{r}
#| label: spatial_4

p1 = DimPlot(sc, reduction="umap.008um", label=TRUE, group.by="seurat_clusters.008um") + NoLegend() + ggtitle("Seurat clustering")
p2 = DimPlot(sc, reduction="banksy_umap.008um", label=TRUE, group.by="banksy_clusters.008um") + NoLegend() + ggtitle("BANKSY clustering")
p1 | p2
```

The same comparison as spatial plots:

```{r}
#| label: spatial_5

p1 = SpatialDimPlot(sc, group.by="seurat_clusters.008um") + NoLegend() + ggtitle("Seurat") + scale_fill_manual(values=cal_pal50)
p2 = SpatialDimPlot(sc, group.by="banksy_clusters.008um") + NoLegend() + ggtitle("BANKSY") + scale_fill_manual(values=cal_pal50)
p1 | p2
```

Note that the BANKSY analysis might not automatically be the better one. You should check the results and determine which analysis make biologically more sense.

## Find spatial domains with BANKSY

By tuning the `lambda` parameter, we can also focus on larger structures, i.e., spatial domains. With bigger `lambda` values and more neighbors, it will include more bins into the analysis. We set `lambda` to 0.8 and the number of neighbors (`k_geom`) to 50. 
Note that we fist delete the BANKSY assay from the previous analysis. This is not strictly necessary but it will save us some memory.

```{r}
#| label: spatial_6

DefaultAssay(sc) = "RNA.008um"
sc = RunBanksy(sc,
               assay="RNA.008um", assay_name="BANKSY.008um",
               features="variable",
               lambda=0.8, k_geom=50,
               verbose=TRUE)

# Set it as default
DefaultAssay(sc) = "BANKSY.008um"

gc()
```

As before, we run PCA, clustering and UMAP with 15 PCs and a clustering resolution of 0.5. Tune these parameters for your data:

```{r}
#| label: spatial_7

sc = RunPCA(sc, reduction.name="banksy_pca.008um", features=rownames(sc[["BANKSY.008um"]]), npcs=10, nfeatures.print=5)
sc = FindNeighbors(sc, reduction="banksy_pca.008um", dims=1:10)
sc = FindClusters(sc, resolution=0.5, algorithm="leiden")
sc$banksy_domains.008um = sc$seurat_clusters
```

Comparison here - why similar? Motivate to try different values

```{r}
#| label: spatial_8

# Remove BANKSY assay and pca since not used anymore and we need the memory.
DefaultAssay(sc) = "RNA.008um"
sc[["BANKSY.008um"]] = NULL
sc[["banksy_pca.008um"]] = NULL
gc()
```

# Cell type annotation by deconvolution

- a
- b

## What is deconvolution?

Spatial transcriptomics methods like Visium HD can contain zero to multiple cells per spot (in our case bin) which might be fully or partially contained, depending on bin resolution, cell size and cell density. Thus we need to run dedicated deconvolution methods to estimate the cell type composition of each bin. In addition, these methods also should take into account the neighborhood of the bins.

One such method is Robust Cell Type Decomposition (RCTD) implemented in the (spacexr)[https://github.com/dmcable/spacexr] package which uses a single-cell reference to deconvolve bin into cells. RCTD learns cell type profiles from the single-cell reference, and uses these to label the spatial transcriptomics bins as cell types.

As reference, we will use the (Allen Brain atlas)[https://portal.brain-map.org/], specifically the single-cell transcriptome dataset reduced to 200,000 cells (and rare cell types removed). Additionally, for this tutorial, we will only annotate the cortex region of the Visium HD dataset.

## Subset the cortex

https://www.dropbox.com/scl/fi/qbs3j1alq33f0qz892ub3/cortex-hippocampus_coordinates.csv?rlkey=lsxglb15jhjdrircy9lb6n0rd&dl=0

We read in the (image) coordinates of the cortex region from a CSV file and convert them into a segmentation object with `CreateSegmentation`. Next we use the `Overlay` function to identify all bins in our spatial dataset that fall into the cortex region as cortex "image": 

```{r}
cortex_coordinates = read.csv("datasets/cortex_coordinates.csv")
cortex_coordinates = CreateSegmentation(cortex_coordinates)

sc[["cortex"]] = Overlay(sc[["slice1.008um"]], cortex_coordinates)
SpatialDimPlot(sc, images="cortex") + NoLegend() + scale_fill_manual(values=cal_pal50)
```

Finally, we create a Seurat object that only includes the cells that are in the cortex region.

```{r}
cortex_bins = Cells(sc[["cortex"]])
cortex = subset(sc, cells=cortex_bins)
cortex[["sketch"]] = NULL
cortex[["pca.sketch"]] = NULL
cortex[["umap.sketch"]] = NULL
cortex[["pca.008um"]] = NULL
cortex[["umap.008um"]] = NULL
cortex[["banksy_umap.008um"]] = NULL
cortex
```

The cortex object still contains ~200000 bins. For computational efficiency, we will sketch the cortex object to reduce the number of bins to 10,000 (FOR THIS TUROIAL) followed by data scaling and PCA dimensionality reduction.

```{r}
# Sketch
cortex = FindVariableFeatures(cortex, selection.method="vst", nfeatures=1000)
cortex = SketchData(
  object = cortex,
  ncells = 10000,
  method = "LeverageScore",
  sketched.assay = "sketch",
  over.write = TRUE
)
# Scale data
cortex = ScaleData(cortex, features=VariableFeatures(cortex))

# PCA
cortex = RunPCA(cortex, reduction.name="pca.sketch", features=VariableFeatures(cortex), npcs=50, nfeatures.print=5)
```

Finally, we need to convert the cortex Seurat object into a format that can be used the spacexr package. We provide the counts, the coordinates of the bins and the total counts per bin:

```{r}
# Counts
counts = GetAssayData(cortex, assay="sketch", layer="counts")

# Coordinates
bins_sketch = Cells(cortex[["sketch"]])
coords = GetTissueCoordinates(cortex)[bins_sketch, 1:2]

# Total counts
total_counts = colSums(counts)

# Create spacexr SpatialRNA object
query = spacexr::SpatialRNA(coords, counts, total_counts)
gc()
```

## Get the reference

The Allen Brain reference dataset has already been downloaded from (here)[https://www.dropbox.com/scl/fi/r1mixf4eof2cot891n215/allen_scRNAseq_ref.Rds?rlkey=ynr6s6wu1efqsjsu3h40vitt7&dl=0] and was saved asSeurat object (`datasets/allen_scRNAseq_ref.Rds`). We will load it:

```{r}
ref = readRDS("datasets/allen_scRNAseq_ref.Rds")
ref
```

The object contains the raw counts as well as precomputed cell annotations. For our analysis we are interested in cell type annotations which can be found in the column `subclass_label`:

```{r}
ref[[]] %>% dplyr::select(subclass_label) %>% head()
```

To use it as reference for deconvolution with RCTD, we need to convert the Seurat object to a spacexr `Reference` object:

```{r}
# Counts
counts = GetAssayData(ref, assay="RNA", layer="counts")

# Cell type labels
cell_types = as.factor(ref$subclass_label)
levels(cell_types) = gsub("/", "-", levels(cell_types))
cell_types = droplevels(cell_types)

# Total counts
total_counts = ref$nCount_RNA

# Create Reference object
# Subsample to 100 cells per cell type as the recommended minimum 
# (https://github.com/dmcable/spacexr/issues/171)
ref = spacexr::Reference(counts, cell_types, total_counts, n_max_cells=100)
gc()
```

## Run deconvolution using RCTD

```{r}
RCTD = create.RCTD(query, ref, max_cores = 25)
gc()
```

```{r}
# Maybedo not run but precompute and let them load it?
RCTD = run.RCTD(RCTD, doublet_mode = "doublet")
```


# Other useful resources

- [OSTA Visium HD Workflow](https://lmweber.org/OSTA/pages/seq-workflow-visium-hd.html)
- [Seurat Visium HD Analysis Vignette](https://satijalab.org/seurat/articles/visiumhd_analysis_vignette)
- [banksy](https://github.com/satijalab/seurat-wrappers/blob/master/docs/banksy.md)


